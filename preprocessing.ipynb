{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "# All imports\n",
    "from pyparsing import Word, hexnums, WordEnd, Optional, alphas, alphanums\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Grid search cross-validation for tuning hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_format = Word(hexnums, exact=8) + WordEnd() # use WordEnd to avoid parsing leading a-f of non-hex numbers as a hex\n",
    "byte_format = Word(hexnums, exact=2) + WordEnd()\n",
    "instrn_line_format = \".text:\" + address_format + (byte_format*(1,))(\"bytes\") + Word(alphas,alphanums)(\"instruction\")\n",
    "byte_line_format = address_format + (byte_format*(1,))(\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "SAMPLES_BASE_DIR = 'trainingData/train/'\n",
    "TRAIN_FILES = ['EZ5uGxW3X8DvKMTcJ6Id','j7UqgpTCiRr1s6kWHvKB','EdwQ1my8754NBkMaJxGA','jRtAmhHUTa3Gud5E07vf','EhCda1DLfb93XTwc8uYJ','jUeRolnr49Mm7DF0JIZX','EiD3lRHWhCw4zY9SjXVI','jZfoAgI4BUzQne1lY5hP','EpBVNQnoMzYaCbe96OZx','jZzi1wtyQf5L7HuTO9hU','EsWOrxC0pMt9jgPQb7cA']\n",
    "# VALIDATE_FILES = []\n",
    "INSTRN_BIGRAM_THRESHOLD = 20\n",
    "BYTE_BIGRAM_THRESHOLD = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(filename):\n",
    "    instrn_unigram = defaultdict(int)\n",
    "    instrn_bigram = defaultdict(int)\n",
    "    byte_unigram = defaultdict(int)\n",
    "    byte_bigram = defaultdict(int)\n",
    "    segments = defaultdict(int)\n",
    "    with open(SAMPLES_BASE_DIR + filename + \".asm\", 'r', encoding='Latin-1') as file:\n",
    "        prev, now = 0, 0\n",
    "        for line in file:\n",
    "            # Filtering lines\n",
    "            segments[line.split(':')[0]] += 1\n",
    "            if not line.startswith('.text'):\n",
    "                continue\n",
    "            if ' db ' in line or ' dd ' in line or ' dw ' in line or 'align ' in line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                result = instrn_line_format.parseString(line)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            prev = now\n",
    "            now = result.instruction\n",
    "            instrn_bigram[(prev, now)] += 1\n",
    "            instrn_unigram[now] += 1\n",
    "#                 if result.instruction == 'CC':\n",
    "#                     print(line)\n",
    "    instrn_bigram = defaultdict(int, {k:v for k,v in instrn_bigram.items() if v > INSTRN_BIGRAM_THRESHOLD and k[0] != 0})\n",
    "#     print(segments)\n",
    "#     print(instrn_unigram)\n",
    "#     print(sum(instrn_unigram.values()))\n",
    "#     print(\"==========================================================================================\")\n",
    "#     print(instrn_bigram)\n",
    "#     print(\"==========================================================================================\")\n",
    "    with open(SAMPLES_BASE_DIR + filename + \".bytes\", 'r', encoding='Latin-1') as file:\n",
    "        prev, now = 0, 0\n",
    "        for line in file:\n",
    "            try:\n",
    "                result = byte_line_format.parseString(line)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            byte_list = list(result.bytes)\n",
    "            for byte in byte_list:\n",
    "                prev = now\n",
    "                now = byte\n",
    "                byte_bigram[(prev, now)] += 1\n",
    "                byte_unigram[now] += 1\n",
    "\n",
    "    byte_bigram = defaultdict(int, {k:v for k,v in byte_bigram.items() if v > BYTE_BIGRAM_THRESHOLD and k[0] != 0})\n",
    "#     print(byte_unigram)\n",
    "#     print(sum(byte_unigram.values()))\n",
    "#     print(\"==========================================================================================\")\n",
    "#     print(byte_bigram)\n",
    "#     print(\"==========================================================================================\")\n",
    "    all_features = copy(segments)\n",
    "    all_features.update(instrn_unigram)\n",
    "    all_features.update(instrn_bigram)\n",
    "    all_features.update(byte_unigram)\n",
    "    all_features.update(byte_bigram)\n",
    "    p = pd.DataFrame(all_features, index=[filename,])\n",
    "#     print(p)\n",
    "    print(filename)\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    label = defaultdict(int)\n",
    "    with open(\"./trainLabels.csv\", 'r') as file:\n",
    "        fileReader = csv.reader(file, delimiter=',')\n",
    "        fileReader = list(fileReader)\n",
    "        fileReader = fileReader[1:]\n",
    "        for row in fileReader:\n",
    "            label[row[0]] = row[1]\n",
    "        trainLabels = {k:v for k,v in label.items() if k in TRAIN_FILES}\n",
    "        trainLabels = pd.DataFrame(list(trainLabels.values()), index = trainLabels.keys(), columns = ['Label'])\n",
    "        print(trainLabels)\n",
    "    return trainLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    train_data_points_ = pd.DataFrame()\n",
    "    for filename in TRAIN_FILES:\n",
    "        features = get_features(filename)\n",
    "        train_data_points_ = pd.concat([train_data_points_, features], axis=0)\n",
    "    train_data_points_.fillna(0, inplace=True)\n",
    "    train_data_labels_ = get_labels()\n",
    "    return (train_data_points_,train_data_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(train_data_):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_train_data_ = scaler.fit_transform(train_data_)\n",
    "    return scaled_train_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearClassifier(train_data_points,train_data_labels):\n",
    "    c = 1\n",
    "    penalty = 'l2'\n",
    "    solver = 'newton-cg'\n",
    "    multi_class = 'ovr'\n",
    "    clf = linear_model.LogisticRegression(penalty=penalty,C=c,solver=solver,multi_class=multi_class)\n",
    "#     gsclf = GridSearchCV(clf, {}, cv=2, scoring='accuracy')\n",
    "#     gsclf.fit(train_data_points,train_data_labels)\n",
    "    clf.fit(train_data_points,train_data_labels)\n",
    "#     return gsclf\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZ5uGxW3X8DvKMTcJ6Id\n",
      "j7UqgpTCiRr1s6kWHvKB\n",
      "EdwQ1my8754NBkMaJxGA\n",
      "jRtAmhHUTa3Gud5E07vf\n",
      "EhCda1DLfb93XTwc8uYJ\n",
      "jUeRolnr49Mm7DF0JIZX\n",
      "EiD3lRHWhCw4zY9SjXVI\n",
      "jZfoAgI4BUzQne1lY5hP\n",
      "EpBVNQnoMzYaCbe96OZx\n",
      "jZzi1wtyQf5L7HuTO9hU\n",
      "EsWOrxC0pMt9jgPQb7cA\n",
      "                     Label\n",
      "EsWOrxC0pMt9jgPQb7cA     8\n",
      "jZfoAgI4BUzQne1lY5hP     9\n",
      "jUeRolnr49Mm7DF0JIZX     2\n",
      "EpBVNQnoMzYaCbe96OZx     4\n",
      "EZ5uGxW3X8DvKMTcJ6Id     1\n",
      "EdwQ1my8754NBkMaJxGA     1\n",
      "jZzi1wtyQf5L7HuTO9hU     3\n",
      "EhCda1DLfb93XTwc8uYJ     1\n",
      "EiD3lRHWhCw4zY9SjXVI     8\n",
      "j7UqgpTCiRr1s6kWHvKB     3\n",
      "jRtAmhHUTa3Gud5E07vf     8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data_points' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4c78970d5760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data_points_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_labels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# validate_data_points_,validate_data_labels_ = get_test_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_points_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_labels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_points' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_points_,train_data_labels_ = get_train_data()\n",
    "# validate_data_points_,validate_data_labels_ = get_test_data()\n",
    "train_data_points_ = scaler(train_data_points_)\n",
    "model = linearClassifier(train_data_points_,train_data_labels_)\n",
    "print(model.best_score_)\n",
    "# accuracy = checkAccuracy(model, validate_data_points_, validate_data_labels)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23045478 0.56809339 0.19373777 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04696673 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.19455253 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04403131 ... 0.         0.         0.        ]\n",
      " [0.17986714 0.         0.02054795 ... 1.         1.         1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=2.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "train_data_points_ = scaler(train_data_points_)\n",
    "model = linearClassifier(train_data_points_,train_data_labels_)\n",
    "# print(model.best_score_)\n",
    "print(train_data_points_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
